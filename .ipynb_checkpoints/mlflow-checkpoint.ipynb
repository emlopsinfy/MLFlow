{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk8s5tUS7Qqe"
   },
   "source": [
    "#mlflow is just for displaying what you have in mlruns folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXH3tzE97dBn"
   },
   "source": [
    "simple mlflow to create experiment and start a run on it  \n",
    "then logs params and metrics on that run  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92yR7fMd7GjQ"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "\n",
    "import mlflow\n",
    "# - mlflow ui - do it in cmd to up mlflow ui\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    mlflow.set_experiment(experiment_name='exp_demo')    \n",
    "    with mlflow.start_run(run_name='run_demo'):\n",
    "        mlflow.log_param('b',2)\n",
    "        for a in range(10):\n",
    "            mlflow.log_metric('a',a)\n",
    "\"\"\"            \n",
    "mlflow.set_experiment(experiment_name='exp_demo1')\n",
    "with mlflow.start_run(run_name='run_demo'):\n",
    "    mlflow.log_param('b',2)\n",
    "    for a in range(10):\n",
    "        mlflow.log_metric('a',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnCmnhhlQjTE"
   },
   "source": [
    "#MLFlow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUxFnN-oQmh6"
   },
   "source": [
    "Setting tracking URI in code.  \n",
    "So the same has to be used while running mlflow in command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "va5DYPmh7nDU"
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlruns.db --port=5555\n",
    "#you can remove the port and it runs in default 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgGQp84ORKiH"
   },
   "source": [
    "First file - s1_data.py  \n",
    "You can run alone or you can use main.py which runs it as the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-7I-pNvRFGs"
   },
   "outputs": [],
   "source": [
    "import os, mlflow, json, argparse, pandas as pd\n",
    "from mlflow.entities import experiment\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "input_dir = os.path.join(os.getcwd(),'input')\n",
    "\n",
    "def load_input(experiment_name, run_name, filename):\n",
    "\n",
    "    #Setting the experiment name from args\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print('Experiment name: {}'.format(experiment_name))\n",
    "\n",
    "    #Getting active experiment details to record in 'info.json' for workflow reference\n",
    "    active_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = active_experiment.experiment_id\n",
    "\n",
    "    #Mlflow tracking\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        #raw input file is stored in artifacts 'data' folder\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        mlflow.log_artifact(filepath, 'data')\n",
    "        print('Input file is saved in mlflow artifact path')\n",
    "\n",
    "\n",
    "        df = pd.read_csv(filepath) \n",
    "        #creating profile report for data analysis - commented out\n",
    "        '''\n",
    "        profile = ProfileReport(df, title='Customer Churn Data', html={'style': {'full_width': True}})\n",
    "        profile.to_file('input/EDA.html')\n",
    "        mlflow.log_artifact('input/EDA.html', 'data')\n",
    "        '''\n",
    "\n",
    "        #Active run details saved in 'info.json'\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        info_dict = {\"run_id\":run_id, \"experiment\":dict(active_experiment), \"filename\":filename}\n",
    "        with open(\"info.json\", \"w\") as f:\n",
    "            json.dump(info_dict, f)\n",
    "\n",
    "        print('run_id is {}'.format(run_id))\n",
    "        print('experiment_id is {}'.format(experiment_id))\n",
    "        print('++++++++++++S1 COMPLETED+++++++++++++')\n",
    "        return run_id, experiment_id\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--run_name', default='test run', required=False)\n",
    "    parser.add_argument('--filename', default='Churn_Modelling.csv')\n",
    "    parser.add_argument('--experiment_name', default='CustomerChurnPrediction')\n",
    "    args = parser.parse_args()\n",
    "    run_id, experiment_id = load_input(args.experiment_name, args.run_name, args.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC-JHD-DRZle"
   },
   "source": [
    "Output  \n",
    "Experiment name: CustomerChurnPrediction  \n",
    "Input file is saved in mlflow artifact path  \n",
    "run_id is 6415949185284117baa391bc72054435  \n",
    "experiment_id is 1  \n",
    "++++++++++++S1 COMPLETED+++++++++++++  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9MXZ31nU23w"
   },
   "source": [
    "First file - s2_preprocessing.py  \n",
    "You can run alone or you can use main.py which runs it as the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G-_zzMRSVWT"
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow, pandas as pd, os, argparse, json\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "client = MlflowClient()\n",
    "\n",
    "temp_dir = os.path.join(os.getcwd(),'temp')\n",
    "\n",
    "\n",
    "def object_to_int(dataframe_series):\n",
    "    cat_dict = {}\n",
    "    if dataframe_series.dtype=='object':\n",
    "        col_name = dataframe_series.name\n",
    "        le = LabelEncoder()\n",
    "        dataframe_series = le.fit_transform(dataframe_series)\n",
    "        le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        cat_dict[col_name] = le_name_mapping\n",
    "        #print(le_name_mapping)\n",
    "    print(cat_dict)\n",
    "    return dataframe_series\n",
    "\n",
    "def split_data(experiment_id, run_id, filename, target):\n",
    "    print('pre-processing start')\n",
    "\n",
    "    with mlflow.start_run(run_id):\n",
    "        filepath = client.get_experiment(experiment_id=experiment_id).artifact_location+'/{}/artifacts/data/{}'.format(run_id,filename)\n",
    "        df = pd.read_csv(filepath, sep=',')\n",
    "\n",
    "        drop_columns = [col for col in df.columns if len(df[col].unique()) == df.shape[0]]\n",
    "        drop_columns.extend(['Surname'])\n",
    "        print('List of columns dropped: {}'.format(drop_columns))\n",
    "        print('====+++++++++====')\n",
    "        df = df.drop(columns=drop_columns, axis=1)\n",
    "\n",
    "        #Apply LabelEncoder to columns of object type\n",
    "        print('\\nData after label encoding:')\n",
    "        print(df.head())\n",
    "        cat_columns = df.columns[df.dtypes=='object']\n",
    "        df[cat_columns] = df[cat_columns].apply(lambda x: object_to_int(x))\n",
    "        print('Data after label encoding:')\n",
    "        print(df.head())\n",
    "\n",
    "        y = df[[target]]\n",
    "        X = df.drop(columns=target, axis=1)\n",
    "        print('Input file features after preprocessing: \\n{}\\n====+++++++++===='.format(df.columns))\n",
    "        print('X features \\n{}\\n====+++++++++===='.format(X.columns))\n",
    "        print('Y features \\n{}\\n====+++++++++===='.format(y.columns))\n",
    "\n",
    "        df.to_csv(os.path.join(temp_dir,'ProcessedData.csv'), index=False)\n",
    "        X.to_csv(os.path.join(temp_dir,'X.csv'), index=False)\n",
    "        y.to_csv(os.path.join(temp_dir,'y.csv'), index=False)\n",
    "\n",
    "        mlflow.log_artifact(local_path=os.path.join(temp_dir,'ProcessedData.csv'), artifact_path='data')\n",
    "        mlflow.log_artifact(local_path=os.path.join(temp_dir,'X.csv'), artifact_path='data')\n",
    "        mlflow.log_artifact(local_path=os.path.join(temp_dir,'y.csv'), artifact_path='data')\n",
    "        \n",
    "        X = pd.read_csv(os.path.join(temp_dir,'X.csv'))\n",
    "        y = pd.read_csv(os.path.join(temp_dir,'y.csv'))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=123, stratify=y)\n",
    "        split_data = [X_train, X_test, y_train, y_test]\n",
    "        filenames = ['X_train.csv', 'X_test.csv', 'y_train.csv', 'y_test.csv']\n",
    "\n",
    "        for filename, splitdata in zip(filenames, split_data):\n",
    "            splitdata.to_csv(os.path.join(temp_dir,filename), index=False)\n",
    "            mlflow.log_artifact(os.path.join(temp_dir,filename), 'data')\n",
    "\n",
    "        print('pre-processing end')\n",
    "        return mlflow.active_run().info.run_id\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--run_id', default=json.load(open('info.json', 'r'))['run_id'])\n",
    "    parser.add_argument('--filename', default=json.load(open('info.json', 'r'))['filename'])\n",
    "    parser.add_argument('--experiment_id', default=json.load(open('info.json', 'r'))['experiment']['experiment_id'])\n",
    "    parser.add_argument('--target', default='Exited')\n",
    "    args = parser.parse_args()\n",
    "    run_id = split_data(args.experiment_id, args.run_id, args.filename, args.target)\n",
    "    print(run_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M814JJ9HVtW7"
   },
   "source": [
    "Output:  \n",
    "pre-processing start\n",
    "List of columns dropped: ['RowNumber', 'CustomerId', 'Surname']\n",
    "====+++++++++====\n",
    "\n",
    "Data after label encoding:\n",
    "   CreditScore Geography  Gender  Age  Tenure  ...  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited\n",
    "0          619    France  Female   42       2  ...              1          1               1        101348.88       1\n",
    "1          608     Spain  Female   41       1  ...              1          0               1        112542.58       0\n",
    "2          502    France  Female   42       8  ...              3          1               0        113931.57       1\n",
    "3          699    France  Female   39       1  ...              2          0               0         93826.63       0\n",
    "4          850     Spain  Female   43       2  ...              1          1               1         79084.10       0\n",
    "\n",
    "[5 rows x 11 columns]\n",
    "{'Geography': {'France': 0, 'Germany': 1, 'Spain': 2}}\n",
    "{'Geography': {'France': 0, 'Germany': 1, 'Spain': 2}}\n",
    "{'Gender': {'Female': 0, 'Male': 1}}\n",
    "Data after label encoding:\n",
    "   CreditScore  Geography  Gender  Age  Tenure  ...  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited\n",
    "0          619          0       0   42       2  ...              1          1               1        101348.88       1\n",
    "1          608          2       0   41       1  ...              1          0               1        112542.58       0\n",
    "2          502          0       0   42       8  ...              3          1               0        113931.57       1\n",
    "3          699          0       0   39       1  ...              2          0               0         93826.63       0\n",
    "4          850          2       0   43       2  ...              1          1               1         79084.10       0\n",
    "\n",
    "[5 rows x 11 columns]\n",
    "Input file features after preprocessing:\n",
    "Index(['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance',\n",
    "       'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n",
    "       'Exited'],\n",
    "      dtype='object')\n",
    "====+++++++++====\n",
    "X features\n",
    "Index(['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance',\n",
    "       'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'],\n",
    "      dtype='object')\n",
    "====+++++++++====\n",
    "Y features\n",
    "Index(['Exited'], dtype='object')\n",
    "====+++++++++====\n",
    "pre-processing end\n",
    "6415949185284117baa391bc72054435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNU-dmmYV5eK"
   },
   "source": [
    "First file - s3_training.py  \n",
    "You can run alone or you can use main.py which runs it as the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj8sRuQWWvM4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import mlflow, pandas as pd, os, argparse, json, time\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "num_fold = 5\n",
    "\n",
    "def train_model(run_id, classifier, **parameters):\n",
    "    print('training start')\n",
    "    artifact_dir = mlflow.get_run(run_id).info.artifact_uri+'/data'\n",
    "    X = pd.read_csv(artifact_dir+'/X.csv')\n",
    "    y = pd.read_csv(artifact_dir+'/y.csv')\n",
    "    x_train = pd.read_csv(artifact_dir+'/X_train.csv')\n",
    "    y_train = pd.read_csv(artifact_dir+'/y_train.csv')\n",
    "    x_test = pd.read_csv(artifact_dir+'/X_test.csv')\n",
    "    y_test = pd.read_csv(artifact_dir+'/y_test.csv')\n",
    "\n",
    "    with mlflow.start_run(run_id = run_id, run_name = str(classifier)):\n",
    "\n",
    "        #print('before SMOTE, target value count is \\n{}'.format(y_train.value_counts()))\n",
    "        over = SMOTE(sampling_strategy='auto', random_state=1234)\n",
    "        x_train, y_train = over.fit_resample(x_train, y_train)\n",
    "        #print('after SMOTE, target value count is \\n{}'.format(y_train.value_counts()))\n",
    "\n",
    "        start = time.time()\n",
    "        model = classifier(**parameters)\n",
    "        sc = StandardScaler()\n",
    "        x_train_scaled = sc.fit_transform(x_train)\n",
    "        x_test_scaled = sc.transform(x_test)\n",
    "        model.fit(x_train_scaled, y_train.values.ravel())\n",
    "\n",
    "        # Get feature importances\n",
    "        # feature_importance = model.feature_importances_\n",
    "        # model_importances = pd.Series(feature_importance,\n",
    "        #                                 index=x_train.columns.values)\n",
    "        # print(model_importances)\n",
    "\n",
    "        end = time.time() - start\n",
    "        print(\"Elapsed time to train model = {} seconds\".format(end))\n",
    "\n",
    "        # Predict the model\n",
    "        predictions = model.predict(x_test_scaled)\n",
    "        print(predictions)\n",
    "        \n",
    "        actual = y_test\n",
    "        pred = predictions\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "\n",
    "        print(\"parameters: {}\".format(parameters))\n",
    "        print(\"  RMSE: %s\" % rmse)\n",
    "        print(\"  MAE: %s\" % mae)\n",
    "        print(\"  R2: %s\" % r2)\n",
    "        print('Accuracy: {}'.format(accuracy))\n",
    "        print('=========+++++++++++===========')\n",
    "\n",
    "        mlflow.log_params(parameters)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        mlflow.sklearn.log_model(model, \"churn-model\")\n",
    "                \n",
    "        print('training end')\n",
    "\n",
    "        # cv = KFold(n_splits=num_fold, random_state=42, shuffle=True)\n",
    "        # recall = cross_val_score(model, X, y, cv=cv, scoring=\"recall\")\n",
    "        # precision = cross_val_score(model, X, y, cv=cv, scoring=\"precision\")\n",
    "        # accuracy = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "        # f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1_macro\")\n",
    "        # print(\"Accuracy in average = {}\".format(np.mean(accuracy)))\n",
    "    \n",
    "        # #Display all metrics in a dataframe\n",
    "        # metrics_df = pd.DataFrame([[accuracy, precision, recall, f1]], columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "        # print(metrics_df)\n",
    "        return model\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    rfc_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"min_samples_leaf\" : 3,\n",
    "    \"max_features\" : \"sqrt\",\n",
    "    }\n",
    "\n",
    "    info = json.load(open('info.json','r'))\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--classifier', default=RandomForestClassifier)\n",
    "    parser.add_argument('--parameters', default=rfc_params)\n",
    "    parser.add_argument('--run_id', default=info['run_id'])\n",
    "    args = parser.parse_args()\n",
    "    run_id = train_model(args.run_id, args.classifier, **args.parameters)\n",
    "    print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqWQpb9kW6OY"
   },
   "source": [
    "Output:  \n",
    "training start\n",
    "Elapsed time to train model = 3.585352897644043 seconds\n",
    "[0 0 0 ... 0 0 1]\n",
    "parameters: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt'}\n",
    "  RMSE: 0.4381780460041329\n",
    "  MAE: 0.192\n",
    "  R2: -0.18454355742491324\n",
    "Accuracy: 0.808\n",
    "=========+++++++++++===========\n",
    "training end\n",
    "RandomForestClassifier(max_features='sqrt', min_samples_leaf=3,\n",
    "                       n_estimators=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FI-adsUW80A"
   },
   "source": [
    "First file - s4_register.py  \n",
    "You can run alone or you can use main.py which runs it as the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUjU8zQds7FY"
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.entities import ViewType, run\n",
    "import os, argparse, json\n",
    "import mlflow.pyfunc\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "client = MlflowClient()\n",
    "\n",
    "def register_model(run_id):\n",
    "  with mlflow.start_run(run_id=run_id):\n",
    "    \n",
    "    active_run_info = mlflow.get_run(run_id=run_id)\n",
    "    active_run_metrics = active_run_info.data.metrics\n",
    "    if active_run_metrics['accuracy'] >= 0.80:\n",
    "        print('Accuracy is greater than 0.80, hence registering the model')\n",
    "        registered_model = mlflow.register_model(\n",
    "        model_uri=\"runs:/{}/churn-model\".format(run_id), name='ChurnModel')\n",
    "\n",
    "        model_name = 'ChurnModel'\n",
    "        stage = 'Production'\n",
    "        production_model = mlflow.pyfunc.load_model(\n",
    "            model_uri=f\"models:/{model_name}/{stage}\"\n",
    "        )\n",
    "        prod_run_info = mlflow.get_run(run_id=production_model.metadata.run_id)\n",
    "        prod_run_metrics = prod_run_info.data.metrics\n",
    "\n",
    "        print('active model accuracy is {}'.format(active_run_metrics['accuracy']))\n",
    "        print('production model accuracy is {}'.format(prod_run_metrics['accuracy']))\n",
    "\n",
    "        if active_run_metrics['accuracy'] > prod_run_metrics['accuracy']:\n",
    "            print('Active model is being transitioned to Production')\n",
    "            client.transition_model_version_stage(\n",
    "              name=registered_model.name,\n",
    "              version=registered_model.version,\n",
    "              stage=\"Production\") \n",
    "        else:\n",
    "            print('Active model is not transitioned to any stage')\n",
    "    else:\n",
    "        print('Active model\\'s accuracy is less than 0.80, hence not registered')\n",
    "\n",
    "    return 'OK'\n",
    "\n",
    "'''\n",
    "    best_models = MlflowClient().search_runs(\n",
    "      experiment_ids=info['experiment']['experiment_id'],\n",
    "      filter_string=\"\",\n",
    "      run_view_type=ViewType.ACTIVE_ONLY,\n",
    "      max_results=2,\n",
    "      order_by=[\"metrics.accuracy DESC\"]\n",
    "    )\n",
    "\n",
    "    best = best_models[0]\n",
    "    best_run_id = best.info.run_id\n",
    "    best_reg_model = mlflow.register_model(\n",
    "        \"runs:/{}/churn-model\".format(best_run_id),\n",
    "        \"churn-pred-model\"\n",
    "    )\n",
    "\n",
    "\n",
    "    client = MlflowClient()\n",
    "    prod_model = client.transition_model_version_stage(\n",
    "        name=best_reg_model.name,\n",
    "        version=best_reg_model.version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "\n",
    "    model_name = prod_model.name\n",
    "    stage = 'Production'\n",
    "\n",
    "    production_model = mlflow.pyfunc.load_model(\n",
    "        model_uri=f\"models:/{model_name}/{stage}\"\n",
    "    )\n",
    "'''\n",
    "\n",
    "if __name__=='__main__':\n",
    "    info = json.load(open('info.json','r'))\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--run_id', default=info['run_id'])\n",
    "    args = parser.parse_args()\n",
    "    production_model = register_model(args.run_id)\n",
    "    print(production_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwxR9uintCGF"
   },
   "source": [
    "Individually ran everything  \n",
    "\n",
    "Now for prediction:  \n",
    "First run model serve in cmd as :  \n",
    "1. set MLFLOW_TRACKING_URI=sqlite:///mlruns.db  \n",
    "2. mlflow models serve --model-uri models:/churn-pred-model/Production -p 1234  \n",
    "since mlflow already in port 5000, given 1234 port for prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbvjrmBjwfqH"
   },
   "source": [
    "now it is hosted in request_uri = 'http://127.0.0.1:1234/invocations'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TtiKrQGwnkq"
   },
   "source": [
    "last file - s5_predict.py  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lpB6HFmws_o"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "#data = '{\"CreditScore\":{\"0\":619,\"1\":608,\"2\":502,\"3\":699,\"4\":850},\"Geography\":{\"0\":0,\"1\":2,\"2\":0,\"3\":0,\"4\":2},\"Gender\":{\"0\":0,\"1\":0,\"2\":0,\"3\":0,\"4\":0},\"Age\":{\"0\":42,\"1\":41,\"2\":42,\"3\":39,\"4\":43},\"Tenure\":{\"0\":2,\"1\":1,\"2\":8,\"3\":1,\"4\":2},\"Balance\":{\"0\":0.0,\"1\":83807.86,\"2\":159660.8,\"3\":0.0,\"4\":125510.82},\"NumOfProducts\":{\"0\":1,\"1\":1,\"2\":3,\"3\":2,\"4\":1},\"HasCrCard\":{\"0\":1,\"1\":0,\"2\":1,\"3\":0,\"4\":1},\"IsActiveMember\":{\"0\":1,\"1\":1,\"2\":0,\"3\":0,\"4\":1},\"EstimatedSalary\":{\"0\":101348.88,\"1\":112542.58,\"2\":113931.57,\"3\":93826.63,\"4\":79084.1}}'\n",
    "#{'Content-Type': 'application/json'}\n",
    "request_uri = 'http://127.0.0.1:1234/invocations'\n",
    "\n",
    "csv_data = 'CreditScore,Geography,Gender,Age,Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary\\r\\n619,0,0,42,2,0.0,1,1,1,101348.88\\r\\n608,2,0,41,1,83807.86,1,0,1,112542.58\\r\\n502,0,0,42,8,159660.8,3,1,0,113931.57\\r\\n699,0,0,39,1,0.0,2,0,0,93826.63\\r\\n850,2,0,43,2,125510.82,1,1,1,79084.1\\r\\n'\n",
    "headers = {'Content-Type': 'text/csv'} \n",
    "\n",
    "if __name__ == '__main__':\n",
    "   try:\n",
    "      #response = requests.post(request_uri, data=data, headers=headers)\n",
    "      response =  requests.post(request_uri, data=csv_data, headers=headers)\n",
    "      print(response.text)\n",
    "      \n",
    "   except Exception as ex:\n",
    "      print(ex)\n",
    "      raise(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZdGYnvrxhXT"
   },
   "source": [
    "Run above all file together with this main.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnH0Jj4Bxk7F"
   },
   "outputs": [],
   "source": [
    "import argparse, mlflow\n",
    "from s1_data import load_input\n",
    "from s2_preprocessing import split_data\n",
    "from s3_training import train_model\n",
    "from s4_register import register_model\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "\n",
    "gbc, rfc, dtc = GradientBoostingClassifier, RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"gamma\": 0,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"silent\" : 1,\n",
    "}\n",
    "\n",
    "gbc_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"learning_rate\" : 0.008,\n",
    "}\n",
    "\n",
    "rfc_params = {\n",
    "    \"n_estimators\": 120,\n",
    "    \"min_samples_leaf\" : 3,\n",
    "    \"max_features\" : \"sqrt\",\n",
    "}\n",
    "\n",
    "rfc_params_hpt = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "dtc_params = {\n",
    "    \"min_samples_leaf\" : 2,\n",
    "}\n",
    "\n",
    "model_param_dict = {gbc: gbc_params, rfc: rfc_params, dtc: dtc_params}\n",
    "\n",
    "def run_pipeline(experiment_name, run_name, filename, target, model, **params):\n",
    "    #Load data from s1\n",
    "    run_id, experiment_id = load_input(experiment_name, run_name, filename)\n",
    "    \n",
    "    #preprocess and split data from s2\n",
    "    split_data(experiment_id, run_id, filename, target)\n",
    "    \n",
    "    #train model from s3\n",
    "    train_model(run_id, model, **params)\n",
    "    \n",
    "    #register the best model from s4\n",
    "    production_model = register_model(run_id)\n",
    "\n",
    "    print(production_model)\n",
    "    return production_model\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--experiment_name', default='CustomerChurnPrediction')\n",
    "    parser.add_argument('--run_name', default='Churn Prediction Test Run')\n",
    "    parser.add_argument('--filename', default='Churn_Modelling.csv')\n",
    "    parser.add_argument('--target', default='Exited')\n",
    "    parser.add_argument('--model', default=rfc)\n",
    "    parser.add_argument('--parameters', default=rfc_params)\n",
    "    args = parser.parse_args()\n",
    "    production_model = run_pipeline(args.experiment_name, args.run_name, args.filename, \n",
    "                                    args.target, args.model, **args.parameters)\n",
    "    # for est_name, est_params in model_param_dict.items():\n",
    "    #     run_pipeline('CustomerChurnPrediction', str(est_name)+' run', \n",
    "    #                     'Churn_Modelling.csv', 'Exited', est_name, **est_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjL6OSBw0Esd"
   },
   "source": [
    "#app.py file for some ui  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7PuQ-LjxPKK"
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, redirect, url_for, flash, jsonify, render_template\n",
    "import numpy as np, pandas as pd\n",
    "import pickle as p\n",
    "import json, mlflow\n",
    "import mlflow.pyfunc\n",
    "from s1_data import load_input\n",
    "#from main import run_pipeline\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "print('HELLO')\n",
    "model_name = \"ChurnModel\"\n",
    "stage = 'Production'\n",
    "model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{stage}\"\n",
    ")\n",
    "print(model)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "_json = [{\"CreditScore\":815.0,\"Geography\":2.0,\"Gender\":0.0,\"Age\":39.0,\"Tenure\":6.0,\"Balance\":0.0,\"NumOfProducts\":1.0,\"HasCrCard\":1.0,\"IsActiveMember\":1.0,\"EstimatedSalary\":85167.88}]\n",
    "df = pd.DataFrame(_json)    \n",
    "predictions = model.predict(df)\n",
    "print(str(predictions))\n",
    "# print(jsonify({\"Predictions\": list(predictions)}))\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_json():\n",
    "    #_json = request.json\n",
    "    if request.method == 'POST':\n",
    "        _json = request.json\n",
    "        #_json = [{\"CreditScore\":815,\"Geography\":2,\"Gender\":0,\"Age\":39,\"Tenure\":6,\"Balance\":0.0,\"NumOfProducts\":1,\"HasCrCard\":1,\"IsActiveMember\":1,\"EstimatedSalary\":85167.88},{\"CreditScore\":714,\"Geography\":2,\"Gender\":0,\"Age\":33,\"Tenure\":10,\"Balance\":103121.33,\"NumOfProducts\":2,\"HasCrCard\":1,\"IsActiveMember\":1,\"EstimatedSalary\":49672.01},{\"CreditScore\":619,\"Geography\":2,\"Gender\":0,\"Age\":38,\"Tenure\":6,\"Balance\":0.0,\"NumOfProducts\":2,\"HasCrCard\":1,\"IsActiveMember\":1,\"EstimatedSalary\":117616.29},{\"CreditScore\":537,\"Geography\":1,\"Gender\":0,\"Age\":37,\"Tenure\":7,\"Balance\":158411.95,\"NumOfProducts\":4,\"HasCrCard\":1,\"IsActiveMember\":1,\"EstimatedSalary\":117690.58},{\"CreditScore\":664,\"Geography\":0,\"Gender\":1,\"Age\":55,\"Tenure\":8,\"Balance\":0.0,\"NumOfProducts\":2,\"HasCrCard\":1,\"IsActiveMember\":1,\"EstimatedSalary\":139161.64}]\n",
    "        df = pd.DataFrame(_json)\n",
    "        df = df.reindex(columns=list(_json[0].keys())) \n",
    "        predictions = model.predict(df)\n",
    "        print(list(predictions))\n",
    "        output = ['yes' if x==1 else 'No' for x in list(predictions)]\n",
    "        #print(jsonify({\"Predictions\": list(predictions)}))\n",
    "        return jsonify({\"Predictions\": list(output)})\n",
    "\n",
    "@app.route('/ui', methods=['GET'])\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def base():\n",
    "    return \"Welcome\"\n",
    "\"\"\"\n",
    "@app.route('/run_pipeline', methods=['POST'])\n",
    "def run_pipeline():\n",
    "    run_name = request.args.get('run_name')\n",
    "    filename = request.args.get('filename')\n",
    "    print(run_name)\n",
    "    print(filename)\n",
    "    run_id = run_pipeline(run_name, filename)\n",
    "    return run_id\n",
    "\"\"\"\n",
    "@app.route('/load_data', methods=['GET', 'POST'])\n",
    "def load_data():\n",
    "    if request.method == 'POST':\n",
    "        run_name = request.args.get('run_name')\n",
    "        filename = request.args.get('filename')\n",
    "        print(run_name)\n",
    "        print(filename)\n",
    "        run_id = load_input(run_name, filename)\n",
    "        return run_id\n",
    "    if request.method == 'GET':\n",
    "        return 'Load data'\n",
    "\n",
    "@app.route('/predict_ui',methods=['POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        CreditScore = int(request.form['CreditScore'])\n",
    "        Geography = request.form['Geography']\n",
    "        Gender = request.form['Gender']\n",
    "        Age = float(request.form['Age'])\n",
    "        Tenure = int(request.form['Tenure'])\n",
    "        Balance = int(request.form['Balance'])\n",
    "        NumOfProducts = int(request.form['NumOfProducts'])\n",
    "        HasCrCard = float(request.form['HasCrCard'])\n",
    "        IsActiveMember = request.form['IsActiveMember']\n",
    "        EstimatedSalary = request.form['EstimatedSalary']\n",
    "        if(Geography == 'France'):\n",
    "            Geography = 0          \n",
    "        elif(Geography == 'Germany'):\n",
    "            Geography = 1\n",
    "        elif(Geography == 'Spain'):\n",
    "            Geography = 2\n",
    "\n",
    "        Gender = request.form['Gender']\n",
    "        if(Gender == 'Female'):\n",
    "            Gender = 0\n",
    "        elif(Gender == 'Male'):\n",
    "            Gender = 1\n",
    "            \n",
    "        prediction = model.predict([[CreditScore,Geography,Gender,Age,Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary]])\n",
    "        print(prediction)\n",
    "        prediction = prediction[0]\n",
    "        if prediction==1:\n",
    "             return render_template('index.html',prediction_text=\"The Customer will leave the bank\")\n",
    "        if prediction==0:\n",
    "             return render_template('index.html',prediction_text=\"The Customer will not leave the bank\")\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=5000)#, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-W0eQif0MKK"
   },
   "source": [
    "#index.html file - keep this in templates folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3dkYvxs0Rnm"
   },
   "outputs": [],
   "source": [
    "\n",
    "<html>\n",
    "  <head>\n",
    "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU\" crossorigin=\"anonymous\">\n",
    "    <style type=\"text/css\">\n",
    "      form, input, select, textarea {\n",
    "      font-size: 15;\n",
    "      }\n",
    "    </style>\n",
    "  </head>\n",
    "\n",
    "<h5 class=\"text-center\">Customer Churn Prediction</h5>\n",
    "\n",
    "<h3>{{ prediction_text }}<h3>\n",
    "\n",
    "<div class=\"w3-container text-center\">\n",
    "<form action=\"{{url_for('predict')}}\" method=\"post\">\n",
    "\n",
    "Credit Score: <input name=\"CreditScore\" type=\"number\"/>\n",
    "</p>\n",
    "<br/>\n",
    "Enter The Location: <select name=\"Geography\" required=\"required\">\n",
    "                      <option value=\"Germany\">Germany</option>\n",
    "                      <option value=\"Spain\">Spain</option>\n",
    "                      <option value=\"France\">France</option>\n",
    "                    </select>\n",
    "<br/>\n",
    "Gender of Customer: <select name=\"Gender\" required=\"required\">\n",
    "                      <option value=\"Male\">Male</option>\n",
    "                      <option value=\"Female\">Female</option>\n",
    "                    </select>\n",
    "<br/>\n",
    "Age: <input name=\"Age\" required=\"required\">\n",
    "<br/>\n",
    "Tenure: <input name=\"Tenure\" required=\"required\">\n",
    "<br/>\n",
    "Enter the Account Balance: <input name=\"Balance\" required=\"required\">\n",
    "<br/>\n",
    "Number of Products: <input name=\"NumOfProducts\" required=\"required\">\n",
    "<br/>\n",
    "Do the Customer have Credit Card? (1=Yes, 0=No): <input name=\"HasCrCard\" required=\"required\">\n",
    "<br/>\n",
    "Is the Customer Active Member (1=Yes, 0=No): <input name=\"IsActiveMember\" required=\"required\">\n",
    "<br/>\n",
    "Enter the Estimated Salary: <input name=\"EstimatedSalary\" required=\"required\">\n",
    "\n",
    "<br/><br/>\n",
    "Predict whether the customer will leave the bank or not?<button type=\"submit\">SUBMIT</button>\n",
    "</div>\n",
    "</form>\t\n",
    "</div>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et39vmeF0LlX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
